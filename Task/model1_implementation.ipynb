{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 > Offensive Language Classification </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary library\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all CSV file\n",
    "train =pd.read_csv(\"../offence_data/train.csv\")\n",
    "test = pd.read_csv(\"../offence_data/test.csv\")\n",
    "valid = pd.read_csv(\"../offence_data/validation.csv\") \n",
    "test_lebel = pd.read_csv(\"../offence_data/test_labels.csv\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 > just visualize train , test, validation data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lebel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\"> adding test label into test data and drop where lebel is NAN  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Index'] = range(1, len(test) + 1)\n",
    "test_lebel['Index'] = range(1, len(test_lebel) + 1)\n",
    "test = test.merge(test_lebel[['Index', 'toxic']], on='Index', how='left')\n",
    "test.drop('Index', axis=1, inplace=True)\n",
    "test['toxic'] = test['toxic'].dropna()\n",
    "test = test.dropna(subset=['toxic'])\n",
    "test['toxic'] = test['toxic'].astype(int)\n",
    "test = test.rename(columns={'content': 'feedback_text'})\n",
    "print(test.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green;\"> Start EDA and Data manipulation   </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lebel destribuition chart\n",
    "label_cols = ['toxic', 'abusive', 'vulgar', 'menace', 'offense', 'bigotry']\n",
    "train[label_cols].sum().plot(kind=\"bar\", title=\"Label Distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Detect laguages present in train dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace this with your real one)\n",
    "# train = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Create a list to hold detected languages\n",
    "detected_languages = []\n",
    "\n",
    "# Iterate through each feedback text\n",
    "for text in train['feedback_text']:\n",
    "    if pd.isna(text):\n",
    "        detected_languages.append(None)\n",
    "    else:\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            detected_languages.append(lang)\n",
    "        except LangDetectException:\n",
    "            detected_languages.append(None)  # Or use a default like 'unknown'\n",
    "\n",
    "# Add the language column to the DataFrame\n",
    "train['lang'] = detected_languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['lang'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lang'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join(train['lang'].astype(str)).lower().split()\n",
    "\n",
    "\n",
    "common_words = Counter(all_words)\n",
    "common_words_df = pd.DataFrame(common_words.most_common(20), columns=['word', 'count'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=common_words_df, x='count', y='word', palette='mako')\n",
    "plt.title(\"Top 20 Most Common Language in training set\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>collecting multi langual stop word for further analisys </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of language names (without language codes)\n",
    "languages = [\n",
    "    'english', 'spanish', 'tagalog', 'estonian', 'german', 'persian (farsi)', 'afrikaans',\n",
    "    'norwegian', 'somali', 'indonesian', 'french', 'vietnamese', 'slovenian', 'turkish',\n",
    "    'portuguese', 'romanian', 'swahili', 'croatian', 'danish', 'albanian', 'welsh', 'italian',\n",
    "    'czech', 'swedish', 'finnish', 'dutch', 'arabic', 'polish', 'bengali', 'catalan', 'hungarian',\n",
    "    'lithuanian', 'slovak', 'russian', 'hebrew', 'korean', 'chinese (simplified)', 'gujarati', \n",
    "    'tamil', 'greek', 'hindi', 'thai', 'latvian', 'macedonian', 'malayalam', 'chinese (traditional)',\n",
    "    'telugu', 'marathi', 'bulgarian'\n",
    "]\n",
    "\n",
    "multi_stop_words = set()\n",
    "\n",
    "# Loop through the list of languages and update the multi_stop_words set\n",
    "for lang in languages:\n",
    "    try:\n",
    "        # Check if stopwords exist for the language\n",
    "        if lang in stopwords.fileids():\n",
    "            multi_stop_words.update(stopwords.words(lang))\n",
    "        else:\n",
    "            print(f\"No stopwords available for: {lang}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with language {lang}: {e}\")\n",
    "\n",
    "print(f\"Collected {len(multi_stop_words)} stopwords.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for Missing Values\n",
    "print(\"Missing values:\\n\", train.isnull().sum())\n",
    "\n",
    "# Plot bar chart for missing values\n",
    "missing_counts = train.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts >= 0]  \n",
    "\n",
    "if not missing_counts.empty:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=missing_counts.index, y=missing_counts.values, palette=\"rocket\")\n",
    "    plt.title(\"Missing Values per Column\")\n",
    "    plt.ylabel(\"Number of Missing Values\")\n",
    "    plt.xlabel(\"Column Name\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Drop rows with missing feedback_text if any\n",
    "missing_texts = train[train['feedback_text'].isnull()]\n",
    "print(f\"\\nRows with missing feedback_text: {len(missing_texts)}\")\n",
    "if len(missing_texts) > 0:\n",
    "    train = train.dropna(subset=['feedback_text'])\n",
    "\n",
    "# Word Distribution and Sentence Length\n",
    "\n",
    "train['word_count'] = train['feedback_text'].apply(lambda x: len(str(x).split()))\n",
    "train['char_count'] = train['feedback_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(train['word_count'], bins=50, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Word Count Distribution')\n",
    "\n",
    "sns.histplot(train['char_count'], bins=50, ax=axes[1], color='salmon')\n",
    "axes[1].set_title('Character Count Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Most Common Words (Before Preprocessing)\n",
    "\n",
    "\n",
    "# Get the list of English stopwords\n",
    "# Remove stop words from feedback_text\n",
    "all_words = ' '.join(train['feedback_text'].astype(str)).lower().split()\n",
    "filtered_words = [word for word in all_words if word not in multi_stop_words]\n",
    "\n",
    "# Count the most common words\n",
    "common_words = Counter(filtered_words)\n",
    "common_words_df = pd.DataFrame(common_words.most_common(20), columns=['word', 'count'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=common_words_df, x='count', y='word', palette='mako')\n",
    "plt.title(\"Top 20 Most Common Words (after removing multi langual  Stopwords only)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to clean the text (remove stopwords, punctuation, numbers, etc.)\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation using str.translate\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers and special characters using regular expressions\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word not in multi_stop_words ]\n",
    "\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the clean_text function to the feedback_text column\n",
    "train['cleaned_feedback_text'] = train['feedback_text'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Combine all cleaned words\n",
    "all_words = ' '.join(train['cleaned_feedback_text']).split()\n",
    "\n",
    "# Count the most common words\n",
    "common_words = Counter(all_words)\n",
    "common_words_df = pd.DataFrame(common_words.most_common(20), columns=['word', 'count'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=common_words_df, x='count', y='word', palette='mako')\n",
    "plt.title(\"Top 20 Most Common Words (After removing stopword and other text noise)\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green;\"> Word Frequency Map  </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all feedback_text into one string\n",
    "all_text = ' '.join(train['feedback_text'].astype(str)).lower()\n",
    "\n",
    "# Generate Word Cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    stopwords=multi_stop_words,\n",
    "    max_words=200,\n",
    "    colormap='plasma'\n",
    ").generate(all_text)\n",
    "\n",
    "# Plot Word Cloud\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"🔤 Word Frequency Map (Word Cloud)\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> balancing the  dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#badly needed but donot get time to do it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>preprocessing , training , evaluation <h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Dataset\n",
    "train_df = train\n",
    "test_df = test  \n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'@\\w+|\\#', '', text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in multi_stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_df['clean_text'] = train_df['feedback_text'].apply(preprocess)\n",
    "test_df['clean_text'] = test_df['feedback_text'].apply(preprocess)\n",
    "\n",
    "# Custom Transformer for Metadata\n",
    "class MetadataExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns].values\n",
    "\n",
    "\n",
    "# Features & Labels\n",
    "\n",
    "text_features = train_df['clean_text']\n",
    "text_test_features = test_df['clean_text']\n",
    "\n",
    "meta_features = ['abusive', 'vulgar', 'menace', 'offense', 'bigotry']\n",
    "train_df[meta_features] = train_df[meta_features].fillna(0)\n",
    "\n",
    "# Fill missing with 0 in test if any of these columns are missing\n",
    "for col in meta_features:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "\n",
    "X = train_df[['clean_text'] + meta_features]\n",
    "y = train_df['toxic']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000), 'clean_text'),\n",
    "    ('meta', StandardScaler(), meta_features)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "\n",
    "# Train Model\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "\n",
    "y_pred = pipeline.predict(X_val)\n",
    "y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\n📊 Evaluation on Validation Set (toxic only):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_val, y_pred_proba):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Toxic\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# AUC-ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Test Set (Toxic)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict and evaluation on Test Set\n",
    "\n",
    "Y_test = test_df['toxic']\n",
    "X_test = test_df[['clean_text'] + meta_features]\n",
    "test_preds = pipeline.predict(X_test)\n",
    "test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ✅ Evaluate on Test Set (if it has 'toxic' labels)\n",
    "if 'toxic' in test_df.columns:\n",
    "    print(\"\\n📊 Evaluation on Test Set (toxic only):\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, test_preds):.4f}\")\n",
    "    print(classification_report(Y_test, test_preds))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(Y_test, test_proba):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix for Test Set\n",
    "    sns.heatmap(confusion_matrix(Y_test, test_preds), annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "    plt.title(\"Confusion Matrix - Test Set (Toxic)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC Curve for test set (on test data)\n",
    "fpr, tpr, thresholds = roc_curve(test_df['toxic'], test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Test Set (Toxic)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> LSTM implementation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'@\\w+|\\#', '', text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in multi_stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = simple_tokenizer(text)\n",
    "        counter.update(tokens)\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_tokens(tokens, vocab, max_len=100):\n",
    "    token_ids = [vocab.get(token, 0) for token in tokens]\n",
    "    return token_ids[:max_len] + [0]*(max_len - len(token_ids[:max_len]))\n",
    "\n",
    "# Dataset Class\n",
    "class CustomLSTMDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, meta=None, vocab=None, max_len=100):\n",
    "        self.texts = [encode_tokens(simple_tokenizer(t), vocab, max_len) for t in texts]\n",
    "        self.labels = labels\n",
    "        self.meta = meta\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        }\n",
    "        if self.meta is not None:\n",
    "            item['meta'] = torch.tensor(self.meta[idx], dtype=torch.float)\n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "# Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, meta_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim + meta_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, meta):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        _, (hidden, _) = self.lstm(embeds)\n",
    "        combined = torch.cat((hidden[-1], meta), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return self.sigmoid(out)\n",
    "# Load and preprocess train data\n",
    "train_df = train\n",
    "train_df['clean_text'] = train_df['feedback_text'].apply(preprocess)\n",
    "\n",
    "X = train_df['clean_text']\n",
    "y = train_df['toxic'].values\n",
    "meta = train_df[['abusive', 'vulgar', 'menace', 'offense', 'bigotry']].values\n",
    "scaler = StandardScaler()\n",
    "meta_scaled = scaler.fit_transform(meta)\n",
    "\n",
    "X_train, X_val, y_train, y_val, meta_train, meta_val = train_test_split(\n",
    "    X, y, meta_scaled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vocab = build_vocab(X_train)\n",
    "\n",
    "train_dataset = CustomLSTMDataset(X_train, y_train, meta_train, vocab)\n",
    "val_dataset = CustomLSTMDataset(X_val, y_val, meta_val, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(len(vocab), 100, 128, meta_dim=5, output_dim=1).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(123):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        meta = batch['meta'].to(device)\n",
    "        labels = batch['label'].unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, meta)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "model.eval()\n",
    "preds, probs, true = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        meta = batch['meta'].to(device)\n",
    "        labels = batch['label'].unsqueeze(1).to(device)\n",
    "        outputs = model(input_ids, meta)\n",
    "        probs.extend(outputs.cpu().numpy())\n",
    "        preds.extend((outputs > 0.5).cpu().numpy())\n",
    "        true.extend(labels.cpu().numpy())\n",
    "\n",
    "true = np.array(true)\n",
    "preds = np.array(preds)\n",
    "probs = np.array(probs)\n",
    "\n",
    "print(\"\\nClassification Report (Validation - Toxic Only):\")\n",
    "print(classification_report(true, preds))\n",
    "print(\"Accuracy:\", accuracy_score(true, preds))\n",
    "print(\"Precision:\", precision_score(true, preds))\n",
    "print(\"Recall:\", recall_score(true, preds))\n",
    "print(\"F1 Score:\", f1_score(true, preds))\n",
    "print(\"ROC AUC:\", roc_auc_score(true, probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(true, preds))\n",
    " \n",
    "# Evaluate on Validation Set graph chart\n",
    "\n",
    "\n",
    "print(\"\\n📊 Evaluation on Validation Set (toxic only):\")\n",
    "print(f\"Accuracy: {accuracy_score(true, preds):.4f}\")\n",
    "print(classification_report(true, preds))\n",
    "print(f\"AUC-ROC: {roc_auc_score(true, preds):.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(true, preds), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Toxic\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# AUC-ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(true, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Test Set (Toxic)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Test Prediction (toxic only)\n",
    "\n",
    "test_df = test\n",
    "test_df['clean_text'] = test_df['feedback_text'].apply(preprocess)\n",
    "test_meta = scaler.transform(test_df[['abusive', 'vulgar', 'menace', 'offense', 'bigotry']].values)\n",
    "\n",
    "test_dataset = CustomLSTMDataset(test_df['clean_text'], meta=test_meta, vocab=vocab)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        meta = batch['meta'].to(device)\n",
    "        outputs = model(input_ids, meta)\n",
    "        predictions = (outputs > 0.5).int().cpu().numpy()\n",
    "        test_preds.extend(predictions.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluation on Test Set\n",
    "\n",
    "Y_test = test_df['toxic']\n",
    "model.eval()\n",
    "T_preds, T_probs, T_true = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        meta = batch['meta'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, meta)\n",
    "        T_probs.extend(outputs.cpu().numpy())\n",
    "        T_preds.extend((outputs > 0.5).cpu().numpy())\n",
    "        T_true.extend(labels.cpu().numpy())\n",
    "\n",
    "T_true = np.array(T_true)\n",
    "T_preds = np.array(T_preds)\n",
    "T_probs = np.array(T_probs)\n",
    "\n",
    "\n",
    "# ✅ Evaluate on Test Set (if it has 'toxic' labels)\n",
    "if 'toxic' in test_df.columns:\n",
    "    print(\"\\n📊 Evaluation on Test Set (toxic only):\")\n",
    "    print(f\"Accuracy: {accuracy_score(Y_test, T_preds):.4f}\")\n",
    "    print(classification_report(Y_test, T_preds))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(Y_test, T_probs):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix for Test Set\n",
    "    sns.heatmap(confusion_matrix(Y_test, T_preds), annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "    plt.title(\"Confusion Matrix - Test Set (Toxic)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC Curve for test set (on test data)\n",
    "fpr, tpr, thresholds = roc_curve(test_df['toxic'], T_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Test Set (Toxic)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
